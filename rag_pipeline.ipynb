{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eb38cefb",
   "metadata": {},
   "source": [
    "# RAG Pipleline for Document proccessing\n",
    "This colab notebook walks us through the entire implementation of RAG pipline for intelligent document processing and smart querying."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e601aa",
   "metadata": {},
   "source": [
    "# Package installation for pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5efa22f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m54.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Install required packages\n",
    "%pip install -q gradio\n",
    "%pip install -q gradio_pdf\n",
    "%pip install -q pypdf PyPDF2 pymupdf\n",
    "%pip install -q sentence-transformers transformers\n",
    "%pip install -q faiss-cpu\n",
    "%pip install -q google-generativeai\n",
    "%pip install -q numpy pandas\n",
    "\n",
    "# Install LlamaIndex packages for enhanced document processing\n",
    "%pip install -q llama-index\n",
    "%pip install -q llama-index-readers-file\n",
    "%pip install -q llama-index-embeddings-huggingface\n",
    "%pip install -q llama-index-vector-stores-faiss\n",
    "%pip install -q llama-index-llms-gemini"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70d6b2f4",
   "metadata": {},
   "source": [
    "# Now to import the core packages installed, and to configure the API key"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4dbc52",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:104: UserWarning: \n",
      "Error while fetching `HF_TOKEN` secret value from your vault: 'Requesting secret HF_TOKEN timed out. Secrets can only be fetched when running from the Colab UI.'.\n",
      "You are not authenticated with the Hugging Face Hub in this notebook.\n",
      "If the error persists, please let us know by opening an issue on GitHub (https://github.com/huggingface/huggingface_hub/issues/new).\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d208a30992824ec59154752306e3948e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5e3093a46d9d4c7a9af1c370f0dd7486",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06c7cb012d184fbb8756d7fe85017c7e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89d158b5f1e04bd59d430f4efbee8043",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760bf9f3d4c543d696319d1078518171",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a9955934bdcc45a68d77752052ac3d70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3d9ec36f95a6406684f60b77c2a81547",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "45517f4c112449bd9029b6f8555a659a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd8adab1d71c4bba88351d2ea7ce7c75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f0d796584fd4c2f976599d28093fbf8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1da6f586de8c43ebbbcac0c118bcb61c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "155d70e334fa47c6b704305efe578a66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4806d68faf974875acd85b59e85dca60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6dca987bc226487a9c9e3d73154a8256",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4b1a56cfe1d04016b3fc935011172c21",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "39ab833f7cdb4b9db8440c019946c013",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4f6d02bff484d1cbc41702b16c61df8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad3bf0f194cb4daeb1de81e1510e2dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ee4a3ed3def42a3a403b03049dbe9bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa1503b768284cc58c60071d9ef9f2ac",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ec99badd615f482996fef81dce61a631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c45f534667a4a43873b16417e7577d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import os\n",
    "from getpass import getpass\n",
    "\n",
    "import gradio as gr\n",
    "from gradio_pdf import PDF\n",
    "import fitz  # PyMuPDF\n",
    "from PyPDF2 import PdfReader\n",
    "import numpy as np\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "import google.generativeai as genai\n",
    "\n",
    "from llama_index.core import Document, VectorStoreIndex, StorageContext\n",
    "from llama_index.core.schema import TextNode\n",
    "from llama_index.embeddings.huggingface import HuggingFaceEmbedding\n",
    "from llama_index.core.node_parser import SentenceSplitter\n",
    "from llama_index.core.vector_stores import MetadataFilters, MetadataFilter, FilterOperator\n",
    "\n",
    "# GEMINI API key\n",
    "GEMINI_API_KEY = os.getenv(\"GEMINI_API_KEY\") or getpass(\"Gemini API key (hidden): \")\n",
    "if GEMINI_API_KEY:\n",
    "    os.environ[\"GEMINI_API_KEY\"] = GEMINI_API_KEY\n",
    "    genai.configure(api_key=GEMINI_API_KEY)\n",
    "    gemini_model = genai.GenerativeModel(\"models/gemini-2.5-flash-lite\")\n",
    "else:\n",
    "    print(\"No GEMINI_API_KEY set ‚Äî set env or enter when prompted.\")\n",
    "\n",
    "# Embeddings\n",
    "embed_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "llama_embed_model = HuggingFaceEmbedding(model_name=\"sentence-transformers/all-MiniLM-L6-v2\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2444bf",
   "metadata": {},
   "source": [
    "# Now to configure the various data classes for the pipeline\n",
    "These are data structures to handle complex document metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d515f681",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PageInfo:\n",
    "    \"\"\"Stores information about a single page\"\"\"\n",
    "    page_num: int\n",
    "    text: str\n",
    "    doc_type: Optional[str] = None\n",
    "    page_in_doc: int = 0\n",
    "\n",
    "@dataclass\n",
    "class LogicalDocument:\n",
    "    \"\"\"Represents a logical document within a PDF\"\"\"\n",
    "    doc_id: str\n",
    "    doc_type: str\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "    chunks: List[Dict] = None\n",
    "\n",
    "@dataclass\n",
    "class ChunkMetadata:\n",
    "    \"\"\"Rich metadata for each chunk\"\"\"\n",
    "    chunk_id: str\n",
    "    doc_id: str\n",
    "    doc_type: str\n",
    "    chunk_index: int\n",
    "    page_start: int\n",
    "    page_end: int\n",
    "    text: str\n",
    "    embedding: Optional[np.ndarray] = None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83a0c233",
   "metadata": {},
   "source": [
    "# Document classification functions and Boundary detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "32710e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def classify_document_type(text: str, max_length: int = 1500) -> str:\n",
    "    \"\"\"\n",
    "    Classify the document type based on its content.\n",
    "    Uses LLM to intelligently identify document category.\n",
    "    \"\"\"\n",
    "    # Truncate text if too long to avoid token limits\n",
    "    text_sample = text[:max_length] if len(text) > max_length else text\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this document and classify it into ONE of these categories:\n",
    "    - Resume: CV, professional profile, work history\n",
    "    - Contract: Legal agreement, terms and conditions, service agreement\n",
    "    - Mortgage Contract: Home loan agreement, mortgage terms, property financing\n",
    "    - Invoice: Bill, payment request, financial statement\n",
    "    - Pay Slip: Salary statement, wage slip, earnings statement\n",
    "    - Lender Fee Sheet: Loan fees, lender charges, closing costs\n",
    "    - Land Deed: Property deed, title document, ownership certificate\n",
    "    - Bank Statement: Account statement, transaction history\n",
    "    - Tax Document: W2, 1099, tax return, tax form\n",
    "    - Insurance: Insurance policy, coverage document\n",
    "    - Report: Analysis, research document, findings\n",
    "    - Letter: Correspondence, memo, communication\n",
    "    - Form: Application, questionnaire, data entry form\n",
    "    - ID Document: Driver's license, passport, identification\n",
    "    - Medical: Medical report, prescription, health record\n",
    "    - Other: Doesn't fit other categories\n",
    "\n",
    "    Document sample:\n",
    "    {text_sample}\n",
    "\n",
    "    Respond with ONLY the category name, nothing else.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        doc_type = response.text.strip()\n",
    "\n",
    "        # Normalize the response\n",
    "        valid_types = [\n",
    "            'Resume', 'Contract', 'Mortgage Contract', 'Invoice', 'Pay Slip',\n",
    "            'Lender Fee Sheet', 'Land Deed', 'Bank Statement', 'Tax Document',\n",
    "            'Insurance', 'Report', 'Letter', 'Form', 'ID Document',\n",
    "            'Medical', 'Other'\n",
    "        ]\n",
    "\n",
    "        # Find best match (case-insensitive)\n",
    "        for valid_type in valid_types:\n",
    "            if doc_type.lower() == valid_type.lower():\n",
    "                return valid_type\n",
    "\n",
    "        return 'Other'\n",
    "    except Exception as e:\n",
    "        print(f\"Classification error: {e}\")\n",
    "        return 'Other'\n",
    "\n",
    "def detect_document_boundary(prev_text: str, curr_text: str,\n",
    "                            current_doc_type: str = None) -> bool:\n",
    "    \"\"\"\n",
    "    Detect if two consecutive pages belong to the same document.\n",
    "    Returns True if they're from the same document.\n",
    "    \"\"\"\n",
    "    # Quick heuristic checks first\n",
    "    if not prev_text or not curr_text:\n",
    "        return False\n",
    "\n",
    "    # Sample the texts for LLM analysis\n",
    "    prev_sample = prev_text[-500:] if len(prev_text) > 500 else prev_text\n",
    "    curr_sample = curr_text[:500] if len(curr_text) > 500 else curr_text\n",
    "\n",
    "    prompt = f\"\"\"\n",
    "    Determine if these two pages are from the SAME document.\n",
    "\n",
    "    Current document type: {current_doc_type or 'Unknown'}\n",
    "\n",
    "    End of Previous Page:\n",
    "    ...{prev_sample}\n",
    "\n",
    "    Start of Current Page:\n",
    "    {curr_sample}...\n",
    "\n",
    "    Consider:\n",
    "    - Continuity of content\n",
    "    - Formatting consistency\n",
    "    - Topic coherence\n",
    "    - Page numbers or headers\n",
    "\n",
    "    Answer ONLY 'Yes' if same document or 'No' if different document.\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = gemini_model.generate_content(prompt)\n",
    "        return response.text.strip().lower().startswith('yes')\n",
    "    except Exception as e:\n",
    "        print(f\"Boundary detection error: {e}\")\n",
    "        # Default to keeping pages together if uncertain\n",
    "        return True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68cd5446",
   "metadata": {},
   "source": [
    "# PDF processing pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "01093771",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_and_analyze_pdf(pdf_file) -> Tuple[List[PageInfo], List[LogicalDocument]]:\n",
    "    \"\"\"\n",
    "    Extract text from PDF and perform intelligent document analysis.\n",
    "    Returns both page-level info and logical document groupings.\n",
    "    Supports various file types including scanned PDFs with OCR.\n",
    "    \"\"\"\n",
    "    print(\"üìñ Starting PDF extraction and analysis...\")\n",
    "\n",
    "    # Extract text from each page\n",
    "    if isinstance(pdf_file, dict) and \"content\" in pdf_file:\n",
    "        doc = fitz.open(stream=pdf_file[\"content\"], filetype=\"pdf\")\n",
    "    elif hasattr(pdf_file, \"read\"):\n",
    "        doc = fitz.open(stream=pdf_file.read(), filetype=\"pdf\")\n",
    "    else:\n",
    "        doc = fitz.open(pdf_file)\n",
    "\n",
    "    pages_info = []\n",
    "    for i, page in enumerate(doc):\n",
    "        text = page.get_text()\n",
    "\n",
    "        # If no text is found,  switch to OCR in case of scanned documents\n",
    "        if not text.strip():\n",
    "            print(f\"  Page {i}: No text found, attempting OCR...\")\n",
    "            try:\n",
    "                # Convert page to image and perform OCR\n",
    "                pix = page.get_pixmap()\n",
    "                img_data = pix.tobytes(\"png\")\n",
    "                from PIL import Image\n",
    "                import pytesseract\n",
    "                import io\n",
    "\n",
    "                img = Image.open(io.BytesIO(img_data))\n",
    "                text = pytesseract.image_to_string(img)\n",
    "                print(f\"  Page {i}: OCR extracted {len(text)} characters\")\n",
    "            except Exception as e:\n",
    "                print(f\"  Page {i}: OCR failed - {e}\")\n",
    "                text = \"\"\n",
    "\n",
    "        pages_info.append(PageInfo(page_num=i, text=text))\n",
    "\n",
    "    doc.close()\n",
    "\n",
    "    if not pages_info:\n",
    "        raise ValueError(\"No text could be extracted from PDF\")\n",
    "\n",
    "    print(f\"‚úÖ Extracted {len(pages_info)} pages\")\n",
    "\n",
    "    # Perform document classification and boundary detection\n",
    "    print(\"üß† Analyzing document structure...\")\n",
    "    logical_docs = []\n",
    "    current_doc_type = None\n",
    "    current_doc_pages = []\n",
    "    doc_counter = 0\n",
    "\n",
    "    for i, page_info in enumerate(pages_info):\n",
    "        if i == 0:\n",
    "            # First page - classify document type\n",
    "            current_doc_type = classify_document_type(page_info.text)\n",
    "            page_info.doc_type = current_doc_type\n",
    "            page_info.page_in_doc = 0\n",
    "            current_doc_pages = [page_info]\n",
    "            print(f\"  Page {i}: New document detected - {current_doc_type}\")\n",
    "        else:\n",
    "            # Check if this page continues the previous document\n",
    "            prev_text = pages_info[i-1].text\n",
    "            is_same = detect_document_boundary(prev_text, page_info.text, current_doc_type)\n",
    "\n",
    "            if is_same:\n",
    "                # Continue current document\n",
    "                page_info.doc_type = current_doc_type\n",
    "                page_info.page_in_doc = len(current_doc_pages)\n",
    "                current_doc_pages.append(page_info)\n",
    "            else:\n",
    "                # New document detected - save previous and start new\n",
    "                logical_doc = LogicalDocument(\n",
    "                    doc_id=f\"doc_{doc_counter}\",\n",
    "                    doc_type=current_doc_type,\n",
    "                    page_start=current_doc_pages[0].page_num,\n",
    "                    page_end=current_doc_pages[-1].page_num,\n",
    "                    text=\"\\n\\n\".join([p.text for p in current_doc_pages])\n",
    "                )\n",
    "                logical_docs.append(logical_doc)\n",
    "                doc_counter += 1\n",
    "\n",
    "                # Start new document\n",
    "                current_doc_type = classify_document_type(page_info.text)\n",
    "                page_info.doc_type = current_doc_type\n",
    "                page_info.page_in_doc = 0\n",
    "                current_doc_pages = [page_info]\n",
    "                print(f\"  Page {i}: New document detected - {current_doc_type}\")\n",
    "\n",
    "    # For the last document\n",
    "    if current_doc_pages:\n",
    "        logical_doc = LogicalDocument(\n",
    "            doc_id=f\"doc_{doc_counter}\",\n",
    "            doc_type=current_doc_type,\n",
    "            page_start=current_doc_pages[0].page_num,\n",
    "            page_end=current_doc_pages[-1].page_num,\n",
    "            text=\"\\n\\n\".join([p.text for p in current_doc_pages])\n",
    "        )\n",
    "        logical_docs.append(logical_doc)\n",
    "\n",
    "    print(f\"‚úÖ Identified {len(logical_docs)} logical documents\")\n",
    "    for ld in logical_docs:\n",
    "        print(f\"   - {ld.doc_type}: Pages {ld.page_start}-{ld.page_end}\")\n",
    "\n",
    "    return pages_info, logical_docs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9031b06b",
   "metadata": {},
   "source": [
    "# Now to implement chunking\n",
    "Using overlapping chunking with a sliding window approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "35cf6b40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def chunk_document_with_metadata(logical_doc: LogicalDocument,\n",
    "                                chunk_size: int = 500,\n",
    "                                overlap: int = 100) -> List[ChunkMetadata]:\n",
    "    \"\"\"\n",
    "    Chunk a logical document while preserving rich metadata.\n",
    "    Uses sliding window with overlap for better context.\n",
    "    \"\"\"\n",
    "    chunks_metadata = []\n",
    "    words = logical_doc.text.split()\n",
    "\n",
    "    if len(words) <= chunk_size:\n",
    "        # Document is small enough to be a single chunk\n",
    "        chunk_meta = ChunkMetadata(\n",
    "            chunk_id=f\"{logical_doc.doc_id}_chunk_0\",\n",
    "            doc_id=logical_doc.doc_id,\n",
    "            doc_type=logical_doc.doc_type,\n",
    "            chunk_index=0,\n",
    "            page_start=logical_doc.page_start,\n",
    "            page_end=logical_doc.page_end,\n",
    "            text=logical_doc.text\n",
    "        )\n",
    "        chunks_metadata.append(chunk_meta)\n",
    "    else:\n",
    "        # Create overlapping chunks\n",
    "        stride = chunk_size - overlap\n",
    "        for i, start_idx in enumerate(range(0, len(words), stride)):\n",
    "            end_idx = min(start_idx + chunk_size, len(words))\n",
    "            chunk_text = ' '.join(words[start_idx:end_idx])\n",
    "\n",
    "            # Calculate which pages this chunk spans\n",
    "            chunk_position = start_idx / len(words)\n",
    "            page_range = logical_doc.page_end - logical_doc.page_start\n",
    "            relative_page = int(chunk_position * page_range)\n",
    "            chunk_page_start = logical_doc.page_start + relative_page\n",
    "            chunk_page_end = min(chunk_page_start + 1, logical_doc.page_end)\n",
    "\n",
    "            chunk_meta = ChunkMetadata(\n",
    "                chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
    "                doc_id=logical_doc.doc_id,\n",
    "                doc_type=logical_doc.doc_type,\n",
    "                chunk_index=i,\n",
    "                page_start=chunk_page_start,\n",
    "                page_end=chunk_page_end,\n",
    "                text=chunk_text\n",
    "            )\n",
    "            chunks_metadata.append(chunk_meta)\n",
    "\n",
    "            if end_idx >= len(words):\n",
    "                break\n",
    "\n",
    "    return chunks_metadata\n",
    "\n",
    "def chunk_with_llama_index(logical_doc: LogicalDocument,\n",
    "                           chunk_size: int = 500,\n",
    "                           chunk_overlap: int = 100) -> List[Document]:\n",
    "    \"\"\"\n",
    "    Alternative: Use LlamaIndex's advanced chunking with metadata.\n",
    "    \"\"\"\n",
    "    # Create LlamaIndex document with metadata\n",
    "    doc = Document(\n",
    "        text=logical_doc.text,\n",
    "        metadata={\n",
    "            \"doc_id\": logical_doc.doc_id,\n",
    "            \"doc_type\": logical_doc.doc_type,\n",
    "            \"page_start\": logical_doc.page_start,\n",
    "            \"page_end\": logical_doc.page_end,\n",
    "            \"source\": f\"{logical_doc.doc_type}_document\"\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Using LlamaIndex's sentence splitter for better chunking\n",
    "    splitter = SentenceSplitter(\n",
    "        chunk_size=chunk_size,\n",
    "        chunk_overlap=chunk_overlap,\n",
    "        paragraph_separator=\"\\n\\n\",\n",
    "        separator=\" \",\n",
    "    )\n",
    "\n",
    "    # Creating nodes (chunks) from document\n",
    "    nodes = splitter.get_nodes_from_documents([doc])\n",
    "\n",
    "    # Converting to the ChunkMetadata format for consistency\n",
    "    chunks_metadata = []\n",
    "    for i, node in enumerate(nodes):\n",
    "        chunk_meta = ChunkMetadata(\n",
    "            chunk_id=f\"{logical_doc.doc_id}_chunk_{i}\",\n",
    "            doc_id=logical_doc.doc_id,\n",
    "            doc_type=logical_doc.doc_type,\n",
    "            chunk_index=i,\n",
    "            page_start=node.metadata.get(\"page_start\", logical_doc.page_start),\n",
    "            page_end=node.metadata.get(\"page_end\", logical_doc.page_end),\n",
    "            text=node.text\n",
    "        )\n",
    "        chunks_metadata.append(chunk_meta)\n",
    "\n",
    "    return chunks_metadata\n",
    "\n",
    "def process_all_documents(logical_docs: List[LogicalDocument],\n",
    "                         use_llama_index: bool = False) -> List[ChunkMetadata]:\n",
    "    \"\"\"\n",
    "    Process all logical documents into chunks with metadata.\n",
    "    Can use either custom or LlamaIndex chunking.\n",
    "    \"\"\"\n",
    "    all_chunks = []\n",
    "\n",
    "    for logical_doc in logical_docs:\n",
    "        if use_llama_index:\n",
    "            chunks = chunk_with_llama_index(logical_doc)\n",
    "        else:\n",
    "            chunks = chunk_document_with_metadata(logical_doc)\n",
    "\n",
    "        logical_doc.chunks = chunks  # Store reference\n",
    "        all_chunks.extend(chunks)\n",
    "        print(f\"üìÑ {logical_doc.doc_type}: Created {len(chunks)} chunks\")\n",
    "\n",
    "    return all_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cd0a0e2",
   "metadata": {},
   "source": [
    "# Query Routing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b7a6b44",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
